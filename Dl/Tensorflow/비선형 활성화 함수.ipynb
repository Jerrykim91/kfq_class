{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시그모이드 : sigmoid\n",
    "    - 실수를 입력으로, 0과 1사이의 값으로 출력\n",
    "    - 큰 음수는 0에 가깝고, 큰 양수는 1에 가깝다\n",
    "    - 오래 동안 사용되어 왔고, 지금은 큰 단점(출력값이 1이나 0에 가까워 지면 기울이가 0이 되는 문제)-> 가중치가 반영이 않되서, 학습이 진행이 않되고, 죽은 뉴런이 된다\n",
    "    - 잘 사용 않됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 탠츠 :  Tanh\n",
    "    - 실수를 입력받고, -1과 1사이의 값으로 출력\n",
    "    -  -1이나 1에 가까와 지면 기울이가 0이 되는 문제를 동일하게 가짐\n",
    "    - 시그모이드보다는 더 많이 사용되엇고, 기울기 소멸이라는 문제는 다소 줄었다 \n",
    "    - 0으로 중심으로 데이터가 만들어지므로, 상대적인 변화값/폭이 더 크게 나온다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 렐루 : ReLU\n",
    "    - 음수면 0, 양수면 그대로\n",
    "    - 최고의 인기를 구가하고 있다 <- 시그모이드의 문제점때문에 이쪽으로 전환\n",
    "    - 거의 모든 아키텍쳐가 사용\n",
    "    - f(x) = max( 0, x )\n",
    "    - 장단점 \n",
    "       - 옵티마이저(경사 하강법)가 더 빠르게 가중치를 찾는데 도움을 준다\n",
    "       - SGD(확률적 경사 하강법)에서 더 빠르게 작동\\\n",
    "       - 연산 비용도 저렴, 임계값만 가질뿐, 시그모이드, 탠츠처럼 연산을 하지 않는다\n",
    "       - 단, 역전파 학습을 진행시, 기울기가 큰값을 가진 경우 응답이 없어지는 문제 -> 죽은 뉴런"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 릭렐루 : Leaky ReLU\n",
    "    - 렐루의 문제점을 개선, 0을 수렴하면 더이상 학습되지 않는 문제를 해결하기위해서\n",
    "    - 음수 입력시 0.01등 작은 수로 반환\n",
    "    - 경우에 따라서는 렐루보다 성능이 더 좋게 나오기도 한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 소프트맥스 : softmax\n",
    "    - 입력받은 값을 0~1사이 값으로 정규화하여 출력\n",
    "    - 출력되는 값들의 총합은 1이 된다\n",
    "    - 3가지 이상의 범주로 분류하는 다향분류에서는 softmax를 많이 사용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
